% -*- coding: UTF-8 -*-

% 导言区，设置文件类型
\documentclass[twoside,nofonts,fancyhdr,openany,UTF8]{ctexbook}
\usepackage{CJK}

\usepackage{amsmath}				% 调用公式宏包
\usepackage{graphicx}				% 调用插图宏包
\usepackage{authblk}				% 添加机构宏包
\usepackage{indentfirst}			% 首行缩进
\usepackage{graphicx}				% 插入图片
\pagestyle{empty}					% 无页眉页脚格式

\graphicspath{ {images/} }		% 指定图片路径
% 让章的前面显示 Lecture x
\CTEXsetup[name={Lecture ,}, number=\arabic{chapter}]{chapter}

% 添加链接宏
\usepackage[colorlinks, linkcolor=red]{hyperref}

% 设置字体
\setCJKmainfont[AutoFakeBold=true]{Adobe Song Std}
\setCJKsansfont{Adobe Heiti Std}
\setCJKmonofont{Adobe FangSong Std}

\title{CS244N Learning Notes}	% book标题

\author{Jerry Shi}			% 作者

%\date{}						% Latex 会自动生成日期，如果不需要，将这个命令加上即可

\affil{HELIX LAB}

% 导言区结束，正文区
\begin{document}

\maketitle					% 制作封面

\tableofcontents 			% 加入目录，包含页码

\mainmatter 				% 让页码从正文部分开始
\renewcommand\thesection{\arabic {section}}

\chapter{Introduction to NLP and Deep Learning}			% Lecture 1 
%1.1
\section{NLP Introduction}
\subsection{What is Nature Language Processing(NLP)?}
在AI概念被广泛提及的今天，不得不提NLP-自然语言处理，那到底什么是NLP，它又有着怎样的目标和划分。
NLP是通向人工智能必不可少的一个环节。NLP是一个多学科交叉的领域，它涵盖了\textbf{Computer Science}、\textbf{Artificial Intelligence}、\textbf{linguistics};做NLP的目的是能够让计算机能够理解“自然语言”从而去做一些有益的事情比如约会，购物和助手比如Siri, Google Assistant, Facebook M, Cortana等。很好的理解自然语言有着很大的困难和挑战后面会具体叙述其原因。

根据NLP研究层次的不同可以进行如下划。

% 图片1
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.3]{nlp_level}
\caption{NLP Level}
\end{figure}

从图中可以看出，最基本的层次分为语音分析和文本OCR、切分，之后是语态分析(分析某些元素所祈祷的作用)，之后是句法分析(对句子中的词语语法功能进行分析)，再下一层是语义解释(分析每个成分在语义含义层面的事情)，最后是语篇分析(所谓的终极理解)。各个问题都是研究的NLP的核心问题，难度和挑战也是逐层加大。
虽然NLP离完全AI还有很大距离，但当前的技术水平，以使它在各行业有了广泛的应用。

% 列表
\begin{itemize}
\item 拼写检查、关键词搜索、同义词查找
\item 信息提取，如提取价格、日期、地点
\item 文本分类、情感分析
\item 机器翻译
\item 对话系统，问答
\end{itemize}
在工业届很多实际场景中也得到广泛的应用。
% 列表
\begin{itemize}
\item 搜索场景(语音和文字)
\item 在线广告
\item 自动、辅助翻译
\item 市场情感分析
\item 语音辨识
\item 聊天对话系统(客服助手、设备控制、购物)
\end{itemize}

%1.2
\subsection{What's so special about human(nature) language?}
人类语言后者称作自然语言被设计的初衷是用来携带、传达彼此之间的信息，并不是由任何物理形成产生的，基于这一个特点，它不同于视觉、图像处理或其他机器学习任务。
举个具体的例子"rocket",这个词可以指代火箭这个概念，同时也可以表示具体事物火箭。不同的说话语气也可以表示不同的含义，比"Whooompaa".自然语言可以用多种形式进行表示，比如声音、手势、书写，这些信息不断的通过信号传送到人的大脑中，这样人才能去理解。
NLP的目标是通过设计算法让计算机能够理解自然语言并帮助执行一些任务。根据任务难度不同，NLP研究任务举例如下。

%列表
\textbf{Easy}
\begin{itemize}
\item 拼写检查
\item 关键词搜索
\item 同义词查找
\end{itemize}

\textbf{Medium}
\begin{itemize}
\item 文档解析
\end{itemize}

\textbf{Hard}
\begin{itemize}
\item 机器翻译
\item 语义分析(分析某个query的具体具体含义)
\item 指代分析(分析一篇文档中指称代词具体指的什么)
\item 问答系统
\item 聊天机器人
\end{itemize}
那么为什么要引入深度学习呢？因为深度学习可以看成是一个强大的表示学习系统，利用学到的representation去解决NLP相关的任务。现在问题来了，如和去得到一个词的表示，这个表示以何种形式进行存在呢？经过前人的探索和研究，将"word"表示成vector形式能够极大的促进任务的解决，因为word都以向量的形式进行表示，则可以进行距离方式(Jaccard, Cosine, Euclidean)进行计算。

% 2节
\section{Word Vector}
将word表示成vector对解决NLP Task有着重要的帮助，如何将word转换为vector呢？最直观的想法是建立一个vocabulary，然后将每个word都表示成一个vocab 大小的vector,vector中只有在word出现的index位置为1，其他位置全部为0，这种方式就是常说的基于BOW(bag of words)的\textbf{one-hot}编码。这种方式的缺陷显而易见，一是vocabulary词典很大，英文词典可达到13million，而中文词典更大；二是这种方式并不能体现同义词之间的相近关系。根据one-hot encoding的结果，计算任意两个词间的点积，结果都是零，这就是体现了这种encoding方式并不能体现出词间的某种相似性。除one-hot encoding之外，还有字符编码等。
% 图片2
%\begin{figure}[!htb]
%\centering
%\includegraphics[scale=0.3]{one_hot_encoding}
%\caption{One-hot encoding}
%\end{figure}

% 3 节
\section{SVD Based Methods}
除了前面提到的one-hot encoding之外还可以利用基于矩阵分解对的方式来获得word vecotr(通常也称作word embedding)。这种方式有个前提假设，“经常同时出现的词语义具有相似性，即词间的共现性。” 首先我们遍历一个大的document库，目的是要构建一个word-document 矩阵。首先构建个vocabulary，然后遍历每个word \emph{i}在每个document \emph{j}中出现的次数，存放在$X_{ij}$,这样就形成了一个矩阵$X_{\emph{R}\emph{M}}$,M是总的文档数量。
当构造出word-doc矩阵之后，利用SVD(Singular Value Decomposition)对矩阵进行分析，矩阵分解公式如下，
%公式自动编号，
\begin{equation}
X = \emph{US$V^T$}
\end{equation}
进行矩阵分解之后，观察奇异值(对角矩阵S中对角线上的值)，然后选取k个值，从而选择子矩阵$U_{1:V, 1:k}$ 作为word embedding 矩阵，那么每个词将表示成一个k-dimension的vector。这中方式其实类似于\textbf{LSA}。但同时这种方式也存在一些问题，如下：

% 列表
\begin{itemize}
\item word-doc矩阵经常变换，因为频繁会有新的词加入进来
\item 整个矩阵会非常系数，因为有很多词并不共现
\item 矩阵维度很大
\item SVD复杂度是O($n^2$)
\item 对于词频率不均衡需(有些词频率会很大)要特殊处理
\end{itemize}


% 4节
\section{Iteration Based Methods - Word2Vec}
换一种思维，设计一种模型，让模型的参数词向量，通过对模型的迭代训练、误差优化、参数更新学到word vectors。在NLP任务中，学者们已尝试过很多中方式来实现这个目的。比如在特定的NLP任务中，刚开始时将每个word转换为一个词向量，训练不仅仅更新模型参数同时也训练word vector。(注：这在tensorflow中是基于look table实现)。这里介绍一种更高效、简单的、概率的方法\textbf{word2vec},它是一个软件包，包含两种算法 \textbf{CBOW}(continuous bag-of-words)和\textbf{Skip-gram}。前者是给定context word 来预测center word，后者刚好相反；两种训练方法\textbf{Negative Sampling} 和\textbf{Hierarchical Softmax},负采样通过负采样来定义目标函数，分层softmax通过一种高效的树结构计算词典中每个词概率来定义目标函数。下文会对他们进行详细阐述。

% 4.1
\subsection{Continuous Bag of Words Model(CBOW)}
给定context word(上下文周边词)来预测center word的方式被称作CBOW模型。CBOW网络结构如图所示。
% 图
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.8]{cbow}
\caption{Continuous bag of words model}
\end{figure}

下面给出模型中一些必要的参数定义和说明：
% 列表
\begin{itemize}
\item $\textbf{\emph{V}}\in{n \times |{V}|}$, 输入词矩阵；n是词向量的embedding size即词向量的维度，|V|是vocabulary的大小，每一列\emph{$v_{i}$}表示$w_{i}$的词向量,维度为$n \times 1$，矩阵中每个元素值是通过模型训练得到。
\item $\textbf{\emph{U}} \in{|V| \times n}$, 输出词矩阵；n、|V|含义同上；每一行\emph{$u_{j}$}表示输出词\emph{$w_j$}的词向量，维度为$n \times 1$
\item \emph{c}代表中心词的下标
\item \emph{m}代表词窗口大小，即中心词前后几个词
\end{itemize}
在CBOW中每个词会学习到两个vector，输入词向量$v_i$和输出词向量$u_j$，最终选择哪个作为后文会讨论。CBOW模型执行具体过程如下.
% 列表
\begin{enumerate}
\item 对给定窗口大小m中的词进行one-hot编码，$(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in R^{|V|})$
\item 通过与输入词矩阵\textbf{\emph{V}}相乘得到词的embedded vectors，$(v_{c-m}=\emph{V}x^{(c-m)}, v_{c-m+1}=Vx^{(c-m+1)},...,v_{c+m}=Vx^{(c+m)} \in R^{n})$
\item 计算context vector的平均值，$\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m} \in R^{n}$
\item 计算vector score $z=U\widehat{v} \in R^{|V|}$,对于两个向量来说，距离越近点积越高，所以为了能够得到较高的分数，这个过程会将两个向量推向更加靠近。
\item 利用softmax进行概率化，$\widehat{y}=softmax{z} \in R^{|V|}$，softmax形式为$p_{i}=\frac{e^{u_i}}{\sum_{j}{}e^{u_j}}$,为什么采用指数形式，目的是转换为大于0的值，分母用来做归一化，是的所有值累计和为1。
\item 我们的目的是希望生成的概率值$\widehat{y} \in R^{|V|}$尽可能的接近真实的概率$y \in R^{|V|}$(one-hot编码而成)
\item 计算loss，这里由于是比较两个概率分布之间的差异，所以我们选择cross entropy $H(\widehat{y}, y)= - \sum_{j=1}^{|V|}y_{j}log(\widehat{y}_{j})$
\item 计算梯度，更新矩阵\textbf{V}和\textbf{U}
\end{enumerate}

以上就是CBOW模型具体执行过程，其中省略了梯度推导和计算.



% 4.2
\subsection{Skip-gram}
Skip-gram是通过中心词来预测周围词的模型。skip-gram网络模型如图所示。
% 图
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.3]{skip-gram}
\caption{Skip gram model}
\end{figure}
skip-gram 模型过程基本与CBOW类似，下面对该过程进行具体介绍。
% 列表
\begin{enumerate}
\item 生成中心词的one-hot词向量$x \in R^{|V|}$作为输入
\item 得到中心词的embedded 向量 $v_{c}=Vx \in R^{n}$, V是输入词矩阵与CBOW含义相同
\item 就是vector score $z=Uv_{c} \in R^{|V|}$, 这里得到的是中心词 m窗口附近的词
\item 概率化，$\widehat{y}=softmax{z} \in R^{|V|}$,这里预测值是$(y_{c-m},...,y_{c-1},...,y_{c+m})$,真实值是one-hot向量
\item 计算损失，这里有个重要假设(Naive Bayes Assumption),给定输入中心词，所有输出词是独立的。因此可以直接利用cross entropy计算误差
\item 计算梯度，更新参数和向量
\end{enumerate}

在CBOW 和Skip gram 模型中，采用的方式是用生成的一种概率分布去预测词的真实分布，如何去评价模型的好坏呢？根据信息论我们知道，cross entropy是用来测度两个分布之间差异，因此在CBOW和SG模型中也采用交叉熵作为目标函数。在SG模型中，基于朴素贝叶斯假设，将预测的上下文词的联合概率分布当做独立分布去计算。这里以SG模型为主，且词窗口设定为1，则目标函数为
$$J = H = -y_{i}log(\widehat{y_{i}}) = -log(p(u_{c}|v_c)) = -log\frac{exp(u_{c}^{T}v_{c})}{\sum_{w=1}^{|V|}exp(u_{w}^{T}v_{c})}$$
其中，$y_i$是真是词向量，属于one-hot编码只有在下标为c处为1，而预测概率$\widehat{y}$是通过softmax而来。以上就是最终的目标函数，可以基于此目标函数对其中的输入参$v_{c}$和输出参数$u_{c}$就梯度，采用梯度下降法进行求解最优值。
各参数梯度求导可参考文献\href{http://shomy.top/2017/07/28/word2vec-all/}{[1]}


% 4.3 
\subsection{Negative Sampling}
回顾一下目标函数，整个词表的大小$\emph{|V|}$ 可能会很大，每次更新都需要$O(\emph{|V|})$的时间，我们设法对它进行优化，一个直观的想法是通过采样来替代遍历整个词典。在每个训练步，通过采样一些负例来代替遍历整个词典。采样原则基于某个分布$P_{n}(w)$，它的概率值与词典中词的频率顺序相对应，换句话说就是频率高的词被采样概率越大。为了适应负采样做法我们需要调整\textbf{目标函数、梯度、更新规则}。
Negative Sampling 是Mikolov在这篇\href{https://arxiv.org/abs/1310.4546}{Paper}中进行论述.负采样是基于Skip-gram模型，但优化的目标函数不同。给定一个词和上下文中的词构成的词对$(w,c)$，用$P(D=1|w,c)$表示词对来源于与语料库(这里语料库的意思是指w是词c的上下文词，即在同一个窗口内)，记$P(D=0|w,c)$表示词对不是来自于语料库，用sigmod函数对前一种情况进行建模
$$P(D=1|w,c,\theta)=\sigma{(v_{c}^{T}v_w)}=\frac{1}{1+exp(-v_{c}^{T}v_w)}$$
备注，sigmod的函数形式为
$$\sigma{(x)}=\frac{1}{1+e^{-x}}$$
函数图像如下，它可以看成是Softmax的1维情况
% image
\begin{figure}
\centering
\includegraphics[scale=0.8]{sigmod}
\caption{Sigmod 函数曲线}
\end{figure}
根据前面的描述，我们设计一个新的目标函数，如果词和上下文来源于同一个语料库，则最大化他们来源于同一个语料库的概率，如果不是，则最大化他们不是来源于同一个语料库的概率，即最小化来源于同一语料库的概率。我们采用极大似然函数法来表示这两个概率，用$\theta$来表示模型参数即输入矩阵$V和U$。
% equation
\begin{align}
\theta &= argmax_{\theta}\prod_{(w,c) \in D}P(D=1|w,c,\theta)\prod_{(w,c) \in \widetilde{D}}P(D=0|w,c,\theta) \\
&= argmax_{\theta}\prod_{(w,c) \in D}P(D=1|w,c,\theta) \prod_{(w,c) \in \widetilde{D}}(1-P(D=1|w,c,\theta)) \\
&= argmax_{\theta}\Sigma_{(w,c) \in D}log P(D=1|w,c,\theta) + \Sigma_{(w,c) \in \widetilde{D}}log(1-P(D=1|w,c,\theta))
\end{align}
最大化似然函数等价于最小化负似然函数。
$$J = - \Sigma_{(w.c) \in D}log{\frac{1}{1+exp(-u_{w}^{T}v_c)}} - \Sigma_{(w,c) \in \widetilde{D}}log{\frac{1}{1+exp(u_{w}^Tv_c)}}$$
其中$\widetilde{D}$是指错误的或者负采样的语料库，比如句子“"stock boil fish is toy”，不能合理组成一个句子，因此它的概率会很低，因此可以通过从词典中随机负例采样构建语料库$\widetilde{D}$。对于skip-gram模型，给定中心词c，那上下文词c-m+j的目标函为
$$-log\sigma{(u_{c-m+j}^{T}\cdot{\widehat{v}})}-\Sigma_{k=1}^{K}log\sigma{(-\widetilde{u}_{k}^{T}\cdot{v_c})}$$
与原始skip-gram模型softmax loss对比
$$-u_{c-m+j}^{T}v_{c}+log\Sigma_{k=1}^{|V|}exp(u_{k}^{T}v_c)$$

对于CBOW模型，给定上下文词$\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m}$对应的中心词$u_c$的目标函数为
$$-log\sigma{(u_{c}^{T}\cdot{\widehat{v}})}-\Sigma_{k=1}^{K}log\sigma{(-\widetilde{u}_{k}^{T}\cdot{\widehat{v}})}$$

原始CBOW softmax loss为：
$$-u_{c}^{T}\widehat{v}+log\Sigma_{j=1}^{|V|}exp{(u_{j}^{T}\widehat{v})} $$

从改进的目标函数和原目标函数对比，可恶意发现，新的损失函数有两点改进 一是遍历从1 to $|V|$ 调整到 1 to K，K远远小于词典大小；第二点是用sigmod代替exp加快计算。
在上面的公式中，$\{\widetilde{u}_k|k=1...K\}$是从$P_n(w)$中进行采样得到，那$P_n(w)$应该是什么样子呢？通常认为最好的分布是基于Unigram Model分布的3/4次幂，为什么是3/4呢？从下面的例子来理解，
% 
\begin{align}
is： 0.9^{3/4} = 0.92 \\
Constitution: 0.09^{3/4} = 0.16 \\
bombastic：0.01^{3/4} = 0.032
\end{align}
其实3/4次幂可以看做是一种平滑技术，因为在进行负采样的时候遵循一个原则，频率高的词被采样的概率高，为了均衡照顾那些频率低的词，所以采用0.75次幂进行提升他们的概率值，同时频率高的词对应的采样概率被稍微降低些。
这种负采样实现的基本思路是，对于一个长度为1的线段，按照每个词的词频将其公平的分配给每个词，每个词分得到长度为：
\begin{align}
len(w)=\frac{Counter(w)}{\Sigma_{u \in D}Counter(u)}
\end{align}
其中counter(w)即是每个词的词频。通过这个式子，我们只需要在0-1直接产生一个随机小数，看其落在线段上的那个区间，就能采样到对应的单词，在word2vec中为了加速计算，通过查表方式实现。将上述线段标上M个“刻度”，刻度之间间隔是相等的，即1/M，现在只需要生成一个0-M间的整数，然后在刻度上查找就可以找到采样的单词，word2vec源码实现中有很多trick来来加速预算。
% 4.4
\subsection{Hierarchical Softmax}
Mikolov在这篇中同时也介绍了另外一个优化普通softmax的方法即使hierarchical softmax，分层softmax。在实践中，hierarchical softmax对罕见词有较好的效果，而negative sampling对高频词有较好的效果。
Hierarchical softmax采用一个二叉树来表示词典中所有词。每一个叶子节点都代表一个词，从根节点到leaf节点只有唯一一条路径，这个路径被定义为每个词的概率，图中除去根节点和叶子节点以外的节点，都是一个需要模型学习的向量。当用Hierarchical softamx时，整个模型中每个词只有输入表示，而不像原始模型中那样也具有输出表示。下图是Hierarchical softmax二叉树的一个示例。
% image
\begin{figure}
\centering
\includegraphics[scale=0.8]{hierarchical}
\caption{Hierarchical softmax二叉树示例}
\end{figure}
在这个模型中，当给定一个向量$w_i$和$P(w|w_i)$时，词w的概率就等于从根节点出发随机游走到与w相关的叶子节点的概率。由于采用二叉树结构，所有模型计算复杂度从最初的$O(|V|)$变成$O(log(|V|))$.
为了进一步介绍该模型，引入一下记号，
% list
\begin{itemize}
\item $L(w)$ 表示从根节点root到叶子节点w路径上所经过的节点个数，比如上图中$L(w_2)=3$。

\item $n(w,i)$ 表示从root到叶子节点w路径上与向量$v_{n(w,i)}$相关的第$i-th$个节点。比如$n(w,1)$表示与$v_{n(w,1)}$相关的第1个节点，其实就是root节点，而$n(w,L(w))$就是与$v_{n(w,L(w))}$相关的第$L(w)$个节点，也即是词w的父节点。

\item $ch(n)$ 表示任意内部节点(出根节点和叶子节点以为的节点)n的任意一个孩子节点(通常指左节点)
\end{itemize}
下面来计算概率，
% equation
\begin{align}
P(w|w_i) = \prod_{j=1}^{L(w)-1} \sigma{([n(w,j+1) = ch(n(w,j))] \cdot v_{n(w,j)}^Tv_{w_i})}
\end{align}

其中，
% 大括号
$$[x] = \left \{ 
\begin{aligned}
1	,if x is true \\
-1	,otherwise
\end{aligned}
\right.
$$
$\sigma(\cdot)$表示sigmoid 函数。
下面具体解释上面概率公式的含义。首先我们计算从$root(n(w,1))$到叶子节点$leaf(w)$路径上所有词的乘积，如果假定ch(n)是节点n的左子节点，那么当路径转向左侧是term$[n(w,j+1) = ch(n(w,j))]$返回1，因为$ch(n(w,j))$表示的是路径上第j个节点的左子节点，$n(w,j+1)$表示这条路径上第j+1个节点，那么他们表示相同的节点；否则term结果为-1，表示转向右子树。
进一步来说，term$[n(w,j+1) = ch(n(w,j))]$提供了一个归一化操作，对于任意一个节点n，把他的所有做子节点和右节点的概率加在一起，对于任意的$v_{n}^{T}v_{w_i}$有
\begin{align}
\sigma(v_{n}^{T}v_{w_i}) + \sigma(-v_{n}^{T}v_{w_i}) = 1
\end{align}
同时该归一化也保证了$\Sigma_{w=1}^{|V|}P(w|w_i)=1$。

最后来比较一个输入向量$v_{w_i}$和内部节点向量$v_{n(w,j)}^{T}$之间的点积相似度。拿图4中的$w_2$为例，从根节点出发为了到达$w_2$需要两次转向左节点，1次转向右节点，那么得到


\begin{align}
 P(w_2|w_i) & =p(n(w_2,1),left)\cdot p(n(w_2,2),left) \cdot p(n(w_2,3),right)  \\
&= \sigma(v_{n(w_2,1)}^{T}v_{w_i}) \cdot \sigma(v_{n(w_2,2)}^{T}v_{w_i}) \cdot \sigma(-v_{n(w_2,3)}^{T}v_{w_i})
\end{align}

为了训练这个模型，我们的目标依然是最小化负对数似然函数$-logP(w|w_i)$，但是我们只需要更新在二叉树中从根节点到叶子节点路径上的节点的向量即可，而不需要更新每个词的输出向量。这个方法的速度取决于如何构建二叉树及如何用叶子节点表示每个词，在MIKOLOV的论文中使用的是Huffman树，它的特点是频率越高的词在树中路径越短。

\section{参考文献}

[Bengio et al., 2003] Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A
neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.
[Collobert et al., 2011] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,
K., and Kuksa, P. P. (2011). Natural language processing (almost) from scratch.
CoRR, abs/1103.0398.
[Mikolov et al., 2013] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient
estimation of word representations in vector space. CoRR, abs/1301.3781.
[Rong, 2014] Rong, X. (2014). word2vec parameter learning explained. CoRR,
abs/1411.2738.
[Rumelhart et al., 1988] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1988).
Neurocomputing: Foundations of research. chapter Learning Representations by
Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA.

\end{document}